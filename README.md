# Exploration-Driven Reinforcement Learning in RPG Video Games: A Case Study of Pokémon Red

This project evaluates the use of direct map exploration as the sole reward of a reinforcement learning training in Pokémon Red, while using a purely visual aproeach, without any RAM accesses. Exploration is measured by the portion of the map an agent has seen. Results show that this reward is enough to defeat the first Gym leader and reach Mt. Moon with the correct setup. At the same time, the resulting agent also works as an automatic mapping agent, keeping track of dialogs, Pokémon encounters and interiors.

## Introduction

Machine learning and artificial intelligence as a whole have recently experienced major advancements. Language models (or LLMs), multimodal models, and diffusion models have granted AI capabilities that once seemed unattainable. These systems are based on neural networks, artificial architectures that mimic the structure and functioning of the human brain to perform computations.

Games like Pokémon serve as effective benchmarks for evaluating AI capabilities for several reasons. Players must follow nonlinear and sometimes maze-like paths, solve puzzles, and reason to make decisions, many of which have long-term consequences. These games are designed with human traits in mind, such as curiosity. The goal is not to create an AI that can complete the game just for the sake of completion, but because doing so implies overcoming a series of challenges that current AI still cannot solve.

However, there are still challenges these systems are unable to overcome or even match the level of a small child. If you use an LLM to play video games where exploration is an intrinsic part of the game, the results are far from human performance. For example, the GPT-4V model struggles to complete even the first few minutes of the game Pokémon Crystal. It has been documented that this model does not exhibit strong spatial reasoning, a key skill for exploration. Recently, during the development of this project, Google announced at its annual conference that its Gemini 2.5 Pro model had managed to complete a full playthrough of Pokémon Red, but it took about 800 hours of gameplay and included a set of additional features implemented by the developer. In contrast, an average human playthrough of Pokémon Red takes approximately 26 hours [(ref)](https://howlongtobeat.com/game/7169).

For developing AI systems in video games, the most suitable approach is reinforcement learning, a branch of AI and machine learning that involves letting the system interact with its environment (in this case, the video game) and rewarding good behaviors while penalizing bad ones. The problem arises when good behaviors are hard to identify or occur infrequently. This situation is commonly defined as a sparse reward environment. In Pokémon, there is a set of actions that could be considered ''good'', but they depend heavily on context or are completely optional. In such cases, exploration can be used as an additional reward.

For a human player, exploring a map is not the main goal of the game but is a major source of motivation. This is called intrinsic reward: the act of exploration is its own reward, unlike something tangible like money. The same idea applies to training with reinforcement learning. Primary goals are rewarded, and intrinsic exploration rewards are added. But how do we measure exploration?

Exploration can be defined as observing or recording new environments. Measuring this is not straightforward, but this work will use the approach of “scanning” and recording the environment. I.e., mapping the surroundings. Intrinsic rewards like exploration have already shown in numerous experiments to improve agent learning in sparse-reward scenarios. However, it is far less common to use it as the only reward during training. Exploration is typically considered a means to an end, not the end goal itself. In projects focused on games like Pokémon, exploration is also used as a reward, but there is little documentation on exactly what capacity it provides to agents. Therefore, this work aims to carry out a series of reinforcement learning training sessions using the PPO (Proximal Policy Optimization) algorithm, with exploration as the sole reward, and perform both quantitative and qualitative analysis of the results. The different training runs will use varying architectures and characteristics to measure their impact on the reward.

Training in this way has the advantage of being more aligned with how real humans play. Other projects combine data extracted from RAM and explicit objectives to guide the agent closely. Obviously, a human player cannot extract memory data nor knows the objectives in advance. Extracting data from RAM involves additional reverse engineering work on the system, which is a recognized limitation even in the state of the art:

> While Pokémon Red serves as a valuable research environment, it comes with several limitations. First, the game is closed-source and requires owning a legal copy. Once a digital copy is available for training, additional engineering effort is needed to extract relevant information from the Game Boy emulator’s RAM. [(ref)](https://arxiv.org/abs/2502.19920)

In contrast, this work will use the game screen as the sole source of information, both for the agent and for calculating the reward, and will rely on this being sufficient for the agent to learn how to progress. If not, this will be documented as well. The reward will be calculated based on the area covered by the AI agents. As the agent moves, the screen data will be used to record the map—effectively mapping the game world similarly to how a robotic vacuum cleaner maps a room it hasn’t seen before. When the robot detects a previously unseen part of the room, it receives a reward.

Training agents for exploration is a meaningful objective in itself. Not only for robotic vacuums as mentioned, but also for search and rescue operations in unknown environments, or mapping inaccessible places like the ocean floor or outer space. Similarly, video game mapping also holds interest for the preservation of old video games for their artistic value, to avoid their loss due to hardware incompatibility or missing source code.

In a previous work by the author, novelty was evaluated as an intrinsic reward, measured by comparing screen image hashes to previously seen ones. It yielded relatively good results in games like Super Mario Land 2, and more modest results in games like the focus of this work, Pokémon Red. Both the implementation and the techniques used in this work represent a considerable improvement over the previous one.

## Objectives

The main objective is to test and document the learning capability provided by exploration as the only reward in reinforcement learning training using the PPO algorithm. The analysis will be conducted at different levels and using various architectures and techniques known to promote exploration in similar scenarios. The development will focus on the game Pokémon Red, but testing will include other games as well. The choice of this game is due to its popularity, the author’s familiarity with it, and its suitability as a sparse-reward environment. There will be a purely numerical analysis of the discovered area, another based on observed behaviors, and another using the Captum interpretability library.

To achieve this, it is necessary to develop a system that correctly maps the game environment from the image, which is a secondary goal of this project. A sufficiently generic system could be used across different games and environments without needing system-specific adaptations, unlike traditional approaches, which often require manually defining reward functions for each case. In the context of video games, this typically involves reverse engineering to locate memory addresses containing reward-relevant data.

The agent developed in this work will also serve as an automatic game-mapping tool, which may be considered another secondary goal.

